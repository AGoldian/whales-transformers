{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "## Augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleTestDataset(Dataset):\n",
    "    def __init__(self, df_with_arrays, transforms=None):\n",
    "        self.df = df_with_arrays\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.df[index]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': img\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1454, 926, 3)\n"
     ]
    }
   ],
   "source": [
    "q = cv2.imread(r'resources\\defaultPhoto.jpg')\n",
    "q = cv2.cvtColor(q, cv2.COLOR_BGR2RGB)\n",
    "print(q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'img_size': 448,\n",
    "    'seed': 22,\n",
    "    'n_fold': 5,\n",
    "    'train_batch_size': 32,\n",
    "    'test_batch_size': 64,\n",
    "    'num_classes': 15587,\n",
    "    'patches_size': 32,\n",
    "    'device': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "}\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, \n",
    "                           scale_limit=0.15, \n",
    "                           rotate_limit=60, \n",
    "                           p=0.5),\n",
    "        A.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"test\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = list(cv2.imread(r'resources\\defaultPhoto.jpg'))\n",
    "test_dataset = HappyWhaleTestDataset(image)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': array([[[121, 121, 121],\n",
       "         [109, 109, 109],\n",
       "         [ 99,  99,  99]],\n",
       " \n",
       "        [[122, 122, 122],\n",
       "         [110, 110, 110],\n",
       "         [100, 100, 100]],\n",
       " \n",
       "        [[122, 122, 122],\n",
       "         [110, 110, 110],\n",
       "         [100, 100, 100]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [105, 105, 105],\n",
       "         [ 86,  86,  86]],\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [105, 105, 105],\n",
       "         [ 86,  86,  86]],\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [105, 105, 105],\n",
       "         [ 86,  86,  86]]], dtype=uint8)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (input_layer): Linear(in_features=3072, out_features=784, bias=True)\n",
       "  (transformer): Sequential(\n",
       "    (0): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=784, out_features=15587, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionTransformer(**{\n",
    "        'embed_dim': 784,\n",
    "        'hidden_dim': 1568,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 6,\n",
    "        'patch_size': 32,\n",
    "        'num_channels': 3,\n",
    "        'num_patches': 196,\n",
    "        'num_classes': 15587,\n",
    "        'dropout': 0.2\n",
    "    }\n",
    ")\n",
    "model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (input_layer): Linear(in_features=3072, out_features=784, bias=True)\n",
       "  (transformer): Sequential(\n",
       "    (0): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=784, out_features=15587, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(r'models\\model-e15.pt', map_location=torch.device('cpu'))['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = list(cv2.imread(r'resources\\defaultPhoto.jpg'))\n",
    "test_dataset = HappyWhaleTestDataset(image, transforms=data_transforms['test'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[-0.0458, -0.0458, -0.0458,  ..., -0.4226, -0.4226, -0.4226],\n",
       "           [-0.0287, -0.0287, -0.0287,  ..., -0.4054, -0.4054, -0.4054],\n",
       "           [ 0.0056,  0.0056,  0.0056,  ..., -0.3712, -0.3712, -0.3712],\n",
       "           ...,\n",
       "           [-0.1657, -0.1657, -0.1657,  ..., -0.6281, -0.6281, -0.6281],\n",
       "           [-0.1999, -0.1999, -0.1999,  ..., -0.6281, -0.6281, -0.6281],\n",
       "           [-0.1999, -0.1999, -0.1999,  ..., -0.6452, -0.6452, -0.6452]],\n",
       " \n",
       "          [[ 0.0826,  0.0826,  0.0826,  ..., -0.3025, -0.3025, -0.3025],\n",
       "           [ 0.1001,  0.1001,  0.1001,  ..., -0.2850, -0.2850, -0.2850],\n",
       "           [ 0.1352,  0.1352,  0.1352,  ..., -0.2500, -0.2500, -0.2500],\n",
       "           ...,\n",
       "           [-0.0399, -0.0399, -0.0399,  ..., -0.5126, -0.5126, -0.5126],\n",
       "           [-0.0749, -0.0749, -0.0749,  ..., -0.5126, -0.5126, -0.5126],\n",
       "           [-0.0749, -0.0749, -0.0749,  ..., -0.5301, -0.5301, -0.5301]],\n",
       " \n",
       "          [[ 0.3045,  0.3045,  0.3045,  ..., -0.0790, -0.0790, -0.0790],\n",
       "           [ 0.3219,  0.3219,  0.3219,  ..., -0.0615, -0.0615, -0.0615],\n",
       "           [ 0.3568,  0.3568,  0.3568,  ..., -0.0267, -0.0267, -0.0267],\n",
       "           ...,\n",
       "           [ 0.1825,  0.1825,  0.1825,  ..., -0.2881, -0.2881, -0.2881],\n",
       "           [ 0.1476,  0.1476,  0.1476,  ..., -0.2881, -0.2881, -0.2881],\n",
       "           [ 0.1476,  0.1476,  0.1476,  ..., -0.3055, -0.3055, -0.3055]]]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_nn(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(test_loader))\n",
    "        x_batch = batch['image']\n",
    "        x_batch = x_batch.to('cpu')\n",
    "        y_test_pred = model(x_batch)\n",
    "        y_test_pred = torch.softmax(y_test_pred, dim = 1)\n",
    "        y_pred_probs, y_pred_tags = torch.topk(y_test_pred, 5, dim = 1)\n",
    "        y_pred_probs = y_pred_probs.cpu().numpy()\n",
    "        y_pred_tags = y_pred_tags.cpu().numpy()\n",
    "        return y_pred_tags[0], y_pred_probs[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14752  1343  6445  5847  4584]\n",
      "[0.02046323 0.01243929 0.01217838 0.01214379 0.01046708]\n"
     ]
    }
   ],
   "source": [
    "tags, probs = inference_nn(next(iter(test_loader)))\n",
    "print(tags)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db = pd.read_csv(r'resources/database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bottlenose_dolphin\n",
       "Name: species, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[db['individual_id'] == 14752]['species'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = []\n",
    "for i in tags:\n",
    "        animals.append(db[db['individual_id'] == i]['species'].mode()[0])\n",
    "popular_animal = db[db['individual_id'] == tags[0]]['species'].mode()\n",
    "x = pd.DataFrame({'ID': animals, 'Prob': probs}, index=[i for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.020463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.012439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.012178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.012144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.010467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID      Prob\n",
       "0  bottlenose_dolphin  0.020463\n",
       "1  bottlenose_dolphin  0.012439\n",
       "2  bottlenose_dolphin  0.012178\n",
       "3  bottlenose_dolphin  0.012144\n",
       "4  bottlenose_dolphin  0.010467"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14752"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
